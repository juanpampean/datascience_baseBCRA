# -*- coding: utf-8 -*-
"""TrabajoFinal Grupo 02.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19O2tBGEZvM8Szra4p-vP7pifg9vp9lQG

INTEGRANTES GRUPO 2 : Delcré,Paola / Piccat, Josefina / López, Juan José / Fernández, Juan Manuel

ACLARACION IMPORTATE: El presente COLAB muestra todos calculos y códigos trabajados en el Trabajo Práctico. Sin embargo no todos se corrieron en Colab sino que se abordaron desde Jupyter Lab para asi no tener problemas de memoría.
"""

#LIBRERÍAS
# Tratamiento de datos
# ==============================================================================
import pandas as pd
import numpy as np
import datetime
import statsmodels.api as sm
import statsmodels.stats as smst
import statsmodels.stats.api as sms

# Gráficos
# ==============================================================================
import matplotlib.pyplot as plt
from matplotlib import style
import seaborn as sbn
import plotly.express as px
import plotly.graph_objects as go

import seaborn as sns 
import statsmodels.api as sm                # modelos estadísticos
import statsmodels.stats.api as sms         # módulo stats de statsmodels

# Preprocesado y modelado
# ==============================================================================
from scipy.stats import pearsonr
from scipy.stats import kurtosis, skew
from scipy.stats import shapiro
from scipy.stats import normaltest




# Configuración matplotlib
# ==============================================================================
plt.rcParams['image.cmap'] = "bwr"
#plt.rcParams['figure.dpi'] = "100"
plt.rcParams['savefig.bbox'] = "tight"
style.use('ggplot') or plt.style.use('ggplot')

# Configuración warnings
# ==============================================================================
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

#df=pd.read_stata('/content/drive/MyDrive/Apuntes diplo/Copia de BCRA202108.dta')
#df=pd.read_stata('BCRA202108.dta')
df=pd.read_stata('/content/drive/MyDrive/DIPLO DATA SCIENCE/tp4/Copia de BCRA/BCRA202108.dta')

"""### Analisis Exploratorio

Revisamos la tabla, realizamos limpieza de variables y valores nulos
"""

df.head(5)

#Observamos la estructura del Dataframe y eliminamos filas con datos nulos (caso cuit)
df.info(null_counts=True)
df.shape

df.isnull().sum()/df.sum()
df.dropna(inplace=True)
#df["cuit"]=df["cuit"].astype(int)
df.isnull().sum()

# Revisamos los períodos informados y eliminamos los meses de junio y julio
df["fecha_informacion"].value_counts()
solo_agosto = df[ (df['fecha_informacion']==202106)
                | (df['fecha_informacion']==202107) ].index
df.drop(solo_agosto, inplace=True)

# Revisamos situación deuda y eliminamos la situación 11 que no corresponde a status de deuda.
df["situacion_deuda"].value_counts()

df.drop(df[df["situacion_deuda"]==11].index, inplace=True)

"""### Agregamos nuevas variables

"""

# Creamos la columna "Morosidad" asignando 1 a Morosos con situación 3 o mayor.
#df['Morosidad'] = np.where(df["situacion_deuda"]>=3, 1, 0)

# Buscado extraer primeros dígitos del CUIL para poder establecer personería de los deudores
df["codigo_cuit"]=round((df["cuit"]/1e9),0)
df["codigo_cuit"].value_counts()

# Eliminamos aquellas filas que contengan los códigos 23,24,28,21 y 25 
# porque son personas físicas pero no permite distinguir el género
a_eliminar=[23,24,28,21,25]
df.drop(df[df["codigo_cuit"].isin([23,24,28,21,25])].index, inplace=True)

df["codigo_cuit"].value_counts()

# Creamos la variable que permita identificar personería jurídica
map_dictionary={20:"MASC",27:"FEM",30:"EMPRESA",33:"EMPRESA",34:"EMPRESA",31:"EMPRESA",35:"EMPRESA"}
df['Tipo personeria'] = df['codigo_cuit'].map(map_dictionary) 
df["Tipo personeria"].value_counts()
#df["Persona"].isnull.value_count
#df["fecha_informacion"].value_counts()

# Buscamos crear la variable DNI para luego estimar rango edades. Estimamos distinguiendo personas fisicas de jurídicas.
df['DNI'] = np.where(df['Tipo personeria']=="EMPRESA", 0 ,round(((df["cuit"]-df["codigo_cuit"]*1e9)/10),0))
#df["DNI"]=round(((df["cuit"]-df["codigo_cuit"]*1e9)/10),0)
df["DNI"]=df["DNI"].astype(int)

# Creamos la variable de Rango Edades, estimando a partir de los números de DNI.
conditionlist = [
    (df['DNI']<=2e7 ) & (df['DNI'] >0) ,
    (df['DNI'] > 2e7) & (df['DNI'] <2.7e7),
    (df['DNI'] > 2.7e7) & (df['DNI'] <3.5e7),
    (df['DNI'] >= 3.5e7)]
choicelist = ['Mas de 50 años','30 a 40 años', '40 a 50 años', 'Hasta 30 años']
df['Rango_edades'] = np.select(conditionlist, choicelist, default='Not Specified')

df['Rango_edades'].value_counts()

# Creamos variable Garantia, si el sumatorio de todas las garantias da un valor mayor a 0 lo calificamos como 1 - "tiene garantia"
df["Posee Garantias"]=np.where((df["garan_otorgadas"]+df["garan_pref_auto_liq"]+df["c_garan_pref_auto_liq"]+df["c_otr_gran_pref"])>0,1,0)

df["Total_Garantias"]=df["garan_otorgadas"]+df["garan_pref_auto_liq"]+df["c_garan_pref_auto_liq"]+df["c_otr_gran_pref"]

df["Tipo personeria"].value_counts()

# Trabajamos sobre variable Entidad:
# Bancos públicos vs privados:
Banca_privada=[7,15,16,17,18,27,34,44,45,72,86,147,65,150,165,191,198,247,254,259,262,266,269,277,281,285,299,301,303,305,310,312,315,319,321,322,325,330,331,332,336,338,339,340,341,386,389,426,431,432]
Banca_publica=[11,14,20,29,60,309,83,93,94,97,268,300,311]
print(type(df["codigo_entidad"]))

df['Tipo_entidad'] = np.where(df['codigo_entidad'].isin(Banca_privada), 'Banca Privada', 'Banca Publica')
df.loc[df["codigo_entidad"]>1000, 'Tipo_entidad'] = 'Otras Entidades'
#df.loc[(round(((df["codigo_entidad"]/1000)-0.5),0)==44) | (round(((df["codigo_entidad"]/1000)-0.5),0)==45) | (round(((df["codigo_entidad"]/1000)-0.5),0)==65), 'Tipo_entidad'] = 'Ent Finaciera Cap Nacional'  
#df.loc[(round(((df["codigo_entidad"]/1000)-0.5),0)==10), 'Tipo_entidad'] = 'Fideicomiso Financiero'
#df.loc[(round(((df["codigo_entidad"]/1000)-0.5),0)==55), 'Tipo_entidad'] = 'Ent No Financiera de Crédito'
#df.loc[(round(((df["codigo_entidad"]/1000)-0.5),0)==50), 'Tipo_entidad'] = 'Sociedad de Garantía Reciproca'
#df.loc[(round(((df["codigo_entidad"]/1000)-0.5),0)==51), 'Tipo_entidad'] = 'Fondos de Garantía de Carácter Público'
#df.loc[(round(((df["codigo_entidad"]/1000)-0.5),0)==70) | (round(((df["codigo_entidad"]/1000)-0.5),0)==71) | (round(((df["codigo_entidad"]/1000)-0.5),0)==72),'Tipo_entidad'] = 'Ent No Financiera emisoras de TC'

df["Tipo_entidad"].value_counts(dropna=False)

df.columns

df["cuit"].nunique()

dfxcuit=[]
dfxcuit=pd.DataFrame(dfxcuit)
dfxcuit=df.groupby(["cuit","Tipo personeria","Rango_edades"])["credito"].sum()
dfxcuit=pd.DataFrame(dfxcuit)
dfxcuit.shape

dfxcuit=dfxcuit.reset_index(["Tipo personeria","Rango_edades"])

dfxcuit.info()

dfxcuit["Credito en mora"]=df[df["situacion_deuda"] > 2].groupby(["cuit"])["credito"].sum()
dfxcuit["Cuentas en mora"]=df[df["situacion_deuda"] > 2].groupby(["cuit"])["cuit"].count()

dfxcuit= dfxcuit.fillna(value= 0
                       )

dfxcuit

dfxcuit["prop_monto_mora"]= (dfxcuit["Credito en mora"]/dfxcuit["credito"])*100

dfxcuit["moroso"]= np.where(dfxcuit["prop_monto_mora"] > 30, 1, 0)

dfxcuit["prop_monto_mora"].value_counts()

dfxcuit= dfxcuit.fillna(value= 0
                       )

NewDFbpr = df[df["Tipo_entidad"] == 'Banca Privada']
dfxcuit["Credito en Banca Privada"]=NewDFbpr.groupby(["cuit"])["credito"].sum()

NewDFbpub = df[df["Tipo_entidad"] == 'Banca Publica']
dfxcuit["Credito en Banca Publica"]=NewDFbpub.groupby(["cuit"])["credito"].sum()

NewDFotr = df[df["Tipo_entidad"] == 'Otras Entidades']
dfxcuit["Credito en otras entidades"]=NewDFotr.groupby(["cuit"])["credito"].sum()

dfxcuit["Monto Garantias"]=df.groupby(["cuit"])["Total_Garantias"].sum()

dfxcuit.fillna(0)

dfxcuit.columns

dfxcuit["Cuentas Totales"]=df.groupby(["cuit"])["cuit"].count()

dfxcuit["Tiene garantias"]=np.where(dfxcuit["Monto Garantias"]>0,1,0)
dfxcuit["Tiene credito en Banca Privada"]=np.where(dfxcuit["Credito en Banca Privada"]>0,1,0)
dfxcuit["Tiene credito en Banca Pública"]=np.where(dfxcuit["Credito en Banca Publica"]>0,1,0)
dfxcuit["Tiene credito en otras Entidades"]=np.where(dfxcuit["Credito en otras entidades"]>0,1,0)

dfxcuit.rename(columns={'Credito en Banca Privada': 'cred_banca_privada', 'Credito en Banca Publica': 'cred_banca_publica',
                       'Credito en otras entidades':'cred_otras_entidades','Rango_edades':'rango_edades',
                       'Tipo personeria':'tipo_personeria','Credito en mora':'credito_en_mora',
                       'Cuentas en mora':'cuentas_mora','Monto Garantias':'monto_garantias','credito':'cred_total',
                       'Cuentas Totales':'cant_cuentas',"Tiene credito en Banca Privada":"tiene_cred_banca_priva",
                       "Tiene credito en Banca Pública":"tiene_cred_banca_pub","Tiene credito en otras Entidades":"tiene_cred_otras_entidades"
                       ,"Tiene garantias":"tiene_garantias"}, inplace=True)

dfxcuit=dfxcuit.fillna(value=0)

dfxcuit.rango_edades.value_counts()

"""###Armamos Dataframe con información por CUIT y no por cuentas en el sistema financiero."""

#EDAD Personas (proxi con primeros 3 dígitos)
#Garantia: Variable Dicotómicas (tiene o no tiene garantia)
#Nueva Columna: Cantidad de Entidades x CUIT
#Código_entidades: Púbicas/Privadas - PENDIENTE PROFE -
#Distribuir pruebas modelos / probar con bases mas chicas - Se tomó una del 10%.
#Optimo---modelo de ML - Classificación (reglogistica-posibilidad de signicancia)
# Randomforest / RedesNeuronales >> No olvidar balancear grupos de entrenamientos
# Pasar a una base x CUIT individual                           HECHO
## Dejamos sólo un mes de análisis¡?--> Dejamos Agosto.        HECHO
#Hacer BOXPLOT: Monto Crédito vs SItuacion Mora (0 o 1)
# Situación ponderada según monto de crédito.

dfxcuit.columns

dfxcuit.shape  ##cuando haya duplicados usamos el siguiente comando df_sinduplicados=df.drop_duplicates(subset="cuit")

dfxcuit=dfxcuit.fillna(value=0)

dfxcuit=dfxcuit.reset_index()
dfxcuit.info()

dfxcuit.drop(dfxcuit.index[dfxcuit["cuit"]==20270000003], inplace = True)

dfxcuit.index[dfxcuit["cuit"]==20270000003]

dfxcuit.columns

dfxcuit=dfxcuit.set_index("cuit")

#dfxcuit.to_stata("global_cuits.dta")
mora1 = df[(df["Tipo_entidad"] == 'Banca Privada') & (df["situacion_deuda"] > 2)]
dfxcuit["mora_en_Banca_Privada"]=mora1.groupby(["cuit"])["credito"].sum()
dfxcuit["tiene_mora_en_Banca_Privada"]=np.where(dfxcuit["mora_en_Banca_Privada"]>0,1,0)
mora2 = df[(df["Tipo_entidad"] == 'Banca Publica') & (df["situacion_deuda"] > 2)]
dfxcuit["mora_en_Banca_Publica"]=mora2.groupby(["cuit"])["credito"].sum()
dfxcuit["tiene_mora_en_Banca_Publica"]=np.where(dfxcuit["mora_en_Banca_Publica"]>0,1,0)
mora3 = df[(df["Tipo_entidad"] == 'Otras Entidades') & (df["situacion_deuda"] > 2)]
dfxcuit["mora_en_Otras_Entidades"]=mora3.groupby(["cuit"])["credito"].sum()
dfxcuit["tiene_mora_en_Otras_Entidades"]=np.where(dfxcuit["mora_en_Otras_Entidades"]>0,1,0)

dfxcuit.fillna(0)
dfxcuit=dfxcuit.reset_index()
dfxcuit.columns

dfxcuit.drop(dfxcuit.index[dfxcuit["tipo_personeria"]=="EMPRESA"], inplace = True)

dfxcuit=dfxcuit.fillna(0)

dfxcuit

dfxcuit.to_stata("global_cuits.dta")

"""# Analisis Descriptivo """

#df_cuit=pd.read_stata('/content/drive/MyDrive/Cintelink/Copia de global_cuits.dta') #jose
df_cuit=pd.read_stata('/content/drive/MyDrive/DIPLO DATA SCIENCE/tp4/global_cuits.dta')

#df_cuit.head(6)

total_banca_priv=df_cuit["cred_banca_privada"].sum()
total_banca_pub=df_cuit["cred_banca_publica"].sum()
total_otras_entidades=df_cuit["cred_otras_entidades"].sum()
total_global=df_cuit["cred_total"].sum()

mora_banca_priv_total=df_cuit["mora_en_Banca_Privada"].sum()
mora_banca_pub_total=df_cuit["mora_en_Banca_Publica"].sum()
mora_otras_ent_total=df_cuit["mora_en_Otras_Entidades"].sum()
mora_total=df_cuit["credito_en_mora"].sum()

total_entidades=[total_banca_priv,total_banca_pub,total_otras_entidades,total_global]
total_mora_x_entidad=[mora_banca_priv_total,mora_banca_pub_total,mora_otras_ent_total,mora_total]
df2=pd.DataFrame(list(zip(total_entidades,total_mora_x_entidad)),columns=["Total Creditos","Total Mora"],index=["Banca Privada","Banca Pública","Otras Entidades","Totales"]

df2["Creditos Normal"]=df2["Total Creditos"]-df2["Total Mora"]
df2["Proporcion Creditos en Mora"]=df2["Total Mora"]/df2["Total Creditos"]*100
df2["Proporcion Creditos en Situacion Normal"]=df2["Creditos Normal"]/df2["Total Creditos"]*100

total_morosos_banca_privada=df_cuit["tiene_mora_en_Banca_Privada"][df_cuit["tiene_mora_en_Banca_Privada"]>0].count()
total_morosos_banca_publica=df_cuit["tiene_mora_en_Banca_Publica"][df_cuit["tiene_mora_en_Banca_Publica"]>0].count()
total_morosos_otras_entidades=df_cuit["Tiene_mora_en_Otras_Entidades"][df_cuit["Tiene_mora_en_Otras_Entidades"]>0].count()


total_cuenta_banca_privada=df_cuit["tiene_cred_banca_priva"][df_cuit["tiene_cred_banca_priva"]>0].count()
total_cuenta_banca_pub=df_cuit["tiene_cred_banca_priva"][df_cuit["tiene_cred_banca_priva"]>0].count()
total_cuenta_otras_entidades=df_cuit["tiene_cred_otras_entidades"][df_cuit["tiene_cred_otras_entidades"]>0].count()	

total_entidades=[total_cuenta_banca_privada,total_cuenta_banca_pub,total_cuenta_otras_entidades]
total_mora_x_entidad=[total_morosos_banca_privada,total_morosos_banca_publica,total_morosos_otras_entidades]
df3=pd.DataFrame(list(zip(total_entidades,total_mora_x_entidad)),columns=["Total Creditos","Total Mora"],index=["Banca Privada","Banca Pública","Otras Entidades"])

df3["Creditos Normal"]=df3["Total Creditos"]-df3["Total Mora"]
df3["Proporcion personas en Mora"]=df3["Total Mora"]/df3["Total Creditos"]*100
df3["Proporcion personas en Situacion Normal"]=df3["Creditos Normal"]/df3["Total Creditos"]*100

df3

plot = pd.crosstab(index=df3['Proporcion personas en Mora']
                  ).apply(lambda r: r/r.sum() *100,
                          axis=0).plot(kind='bar', stacked=True)

# Se trabajó sobre la distribución de la variable crédito, se analizó la posibilidad de eliminar los valores atípicos 
# utilizando la regla de los cuartiles pero al tratarse de una distribución con asimetría derecha nos dejaba con valores hasta $354.000,00.-
# Por lo tanto se decide trabajar con deudores con crédito hasta 10.000.000,00.-
#(donde se eliminan aquellos valores que se encuentre +- 1.5IQR, siendo IQR la región entre el segundo y el tercer cuartil)

df_cuit["cred_total"].hist(bins=1000,range=[0,10000])
plt.ticklabel_format(style='plain')
plt.xticks(rotation=45 )
plt.show()

def get_iqr_values(df_in, col_name):
    median = df_in[col_name].median()
    q1 = df_in[col_name].quantile(0.25) # 25th percentile / 1st quartile
    q3 = df_in[col_name].quantile(0.75) # 7th percentile / 3rd quartile
    iqr = q3-q1 #Interquartile range
    minimum  = q1-1.5*iqr # The minimum value or the |- marker in the box plot
    maximum = q3+1.5*iqr # The maximum value or the -| marker in the box plot
    return median, q1, q3, iqr, minimum, maximum

get_iqr_values(df_cuit, "cred_total")

# Filtramos y quitamos con una máscara aquellos cuit con créditos superiores a los 10 Millones.
df_cuit=df_cuit[df_cuit["cred_total"]<10000]

df_cuit.shape[0]

"""# Analisis Univariado

###Variables catégoricas
"""

df_categoricas= df_cuit[["tipo_personeria", "rango_edades","tiene_cred_banca_priva","tiene_cred_banca_pub","tiene_cred_otras_entidades","tiene_garantias","tiene_mora_en_Banca_Privada","tiene_mora_en_Banca_Publica","Tiene_mora_en_Otras_Entidades"]]

df_categoricas.nunique()

"""**Variable Tipo de Personería**"""

df_categoricas.tipo_personeria.value_counts()

print (
    "Frecuencias Relativas"
         )
100 * df_categoricas.tipo_personeria.value_counts() / len(df_categoricas['tipo_personeria'])

df_pastel = df_categoricas[['tipo_personeria']]

#Datos
data = df_categoricas.tipo_personeria.value_counts()
labels = ['MASC', 'FEM']

#PAleta de colores
colors = sbn.color_palette('pastel')[0:5]

#Gráfico de torta
plt.pie(data, labels = labels, colors = colors, autopct='%.0f%%')
plt.title("Tipo de personería")
plt.show()

"""Del gráfico se desprende que existe una relacion pareja entre cantidad de personas con género masculino y femenino que poseen créditos en el sistema financiero.

**Variable Rango de edades**
"""

df_categoricas.rango_edades.value_counts()

print (
    "Frecuencias Relativas"
         )
100 * df_categoricas.rango_edades.value_counts()[:10] / len(df_categoricas['rango_edades'])

plot = df_categoricas.rango_edades.value_counts().plot(kind='bar',
                                            title='Rango de edades')

"""Podemos observar que el rango etario que predomina es el de mayores de 50 años, seguido por el de 40 a 50 años, 30 a 40 años y en último lugar los menores de 30.

# **Analisis variables numericas**

---
"""

df_numericas = [["cred_total","credito_en_mora","prop_monto_mora","cred_banca_privada","cred_banca_publica	","cred_otras_entidades","	monto_garantias","cant_cuentas","mora_en_Banca_Privada","mora_en_Banca_Publica","mora_en_Otras_Entidades"]]

desc = df_numericas.describe()
desc.loc['count'] = desc.loc['count'].astype(int).astype(str)
desc.iloc[1:] = desc.iloc[1:].applymap('{:.5f}'.format)
desc

plt.plot(df_numericas["credito_en_mora"])

sns.boxplot(x="cred_banca_privada" , data=df_numericas, palette="Set1")

df_numericas.cred_banca_privada.sum()

sns.boxplot(x="cred_total", data=df_numericas, palette="Set1",orient="h",)

sns.boxplot(x="monto_garantias", data=df_numericas, palette="Set1",orient="h")

sns.boxplot(x="mora_en_Banca_Privada" , data=df_numericas, palette="Set1")

plt.rcParams['figure.figsize'] = (15, 8) 
sns.heatmap(df_numericas.corr(method='pearson'), cmap = 'Wistia', annot = True)
plt.title('Heatmap for the Data', fontsize = 20)
plt.show()

"""# Analisis bivariado"""

pd.crosstab(df_cuit.moroso,df_cuit.rango_edades)

ct = pd.crosstab(df_cuit_personas.moroso, df_cuit_personas.rango_edades)
    
ax = ct.plot(kind='bar', stacked=True, rot=0)
ax.legend(title='Rango de Edades', bbox_to_anchor=(1, 1.02), loc='upper left')

pers=df_cuit_personas["tipo_personeria"].unique().tolist()
for i in pers:
  filtro = df_cuit_personas['tipo_personeria'] == i
  df_g = df_cuit_personas[filtro]
  etiqueta = "Grupo " + i
  labels = ['Normal','En Mora']
  df_g["moroso"].value_counts().plot(kind='pie',autopct="%1.1f%%",labels=labels,title=etiqueta)
  
  plt.show()

pd.crosstab(df_cuit_personas.tipo_personeria,df_cuit_personas.moroso,margins=True)

df_cuit_personas.shape

pd.crosstab(index=df_categoricas['tipo_personeria'], columns=df_categoricas['rango_edades'], 
            ).apply(lambda r: r/r.sum(),
                                axis=1) #Axis=0 Totales columnas

# Gráfico de barras de personería según rango de edades
plot = pd.crosstab(index=df_categoricas['tipo_personeria'],
            columns=df_categoricas['rango_edades']).apply(lambda r: r/r.sum() *100,
                                              axis=1).plot(kind='bar')

pd.crosstab(index=df_categoricas["tiene_cred_banca_priva"], columns=df_categoricas['rango_edades'], 
            ).apply(lambda r: r/r.sum(),
                                axis=1) #Axis=0 Totales columnas

df_cuit_personas.groupby("tipo_personeria")["cred_total"].sum()

plot = df_cuit_personas.groupby("tipo_personeria")["cred_total"].sum().plot(kind='pie', autopct='%.2f%%', 
                                            figsize=(5, 5),
                                            title='Tiene crédito total')

df_cuit_personas.groupby("rango_edades")["credito_en_mora"].sum()

plot = df_cuit.groupby("rango_edades")["credito_en_mora"].sum().plot(kind='bar',
                                            title='Rango de edades')

plot = df_cuit_personas.groupby("tipo_personeria")["credito_en_mora"].sum().plot(kind='pie', autopct='%.2f%%', 
                                            figsize=(5, 5),
                                            title='Tiene crédito en mora')

df_cuit_personas.groupby("tipo_personeria")["credito_en_mora"].sum()

pd.crosstab(index=df_categoricas["tiene_cred_banca_priva"], columns=df_categoricas['rango_edades'], 
            ).apply(lambda r: r/r.sum(),
                                axis=1) #Axis=0 Totales columnas

df_cuit_personas.groupby("rango_edades")["mora_en_Banca_Privada"].sum()

grandes_deudores=df_cuit_personas[df_cuit_personas["cred_total"]<500]

sns.boxplot(grandes_deudores["cred_total"])

grandes_deudores.shape

pd.crosstab(grandes_deudores.moroso,grandes_deudores.tipo_personeria,margins=True,normalize='columns',)\
  .round(4)*100

pd.crosstab(grandes_deudores.moroso,grandes_deudores.tipo_personeria,margins=True)

grandes_deudores.sort_values(by="Monto_Garantias",ascending=False).head(5)

variables_cuant_gd=grandes_deudores[["Cuentas_en_mora","Credito_en_Banca_Privada",
                                         "Credito_en_Banca_Publica","Credito_en_otras_entidades",
                                          "Monto_Garantias","Cuentas_Totales","credito"]]

desc = variables_cuant_gd.describe()
desc.loc['count'] = desc.loc['count'].astype(int).astype(str)
desc.iloc[1:] = desc.iloc[1:].applymap('{:.5f}'.format)
desc

def get_iqr_values(df_in, col_name):
    median = df_in[col_name].median()
    q1 = df_in[col_name].quantile(0.25) # 25th percentile / 1st quartile
    q3 = df_in[col_name].quantile(0.75) # 7th percentile / 3rd quartile
    iqr = q3-q1 #Interquartile range
    minimum  = q1-1.5*iqr # The minimum value or the |- marker in the box plot
    maximum = q3+1.5*iqr # The maximum value or the -| marker in the box plot
    return median, q1, q3, iqr, minimum, maximum

def get_iqr_text(df_in, col_name):
    median, q1, q3, iqr, minimum, maximum = get_iqr_values(df_in, col_name)
    text = f"median={median:.2f}, q1={q1:.2f}, q3={q3:.2f}, iqr={iqr:.2f}, minimum={minimum:.2f}, maximum={maximum:.2f}"
    return text

def remove_outliers(df_in, col_name):
    _, _, _, _, minimum, maximum = get_iqr_values(df_in, col_name)
    df_out = df_in.loc[(df_in[col_name] > minimum) & (df_in[col_name] < maximum)]
    return df_out

def count_outliers(df_in, col_name):
    _, _, _, _, minimum, maximum = get_iqr_values(df_in, col_name)
    df_outliers = df_in.loc[(df_in[col_name] <= minimum) | (df_in[col_name] >= maximum)]
    return df_outliers.shape[0]

def box_and_whisker(df_in, col_name):
    title = get_iqr_text(df_in, col_name)
    sns.boxplot(df_in[col_name])
    plt.title(title)
    plt.show()

df_in=df_cuit_personas
col_name="credito"
get_iqr_values(df_in, col_name)
get_iqr_text(df_in, col_name)
remove_outliers(df_in, col_name)
count_outliers(df_in, col_name)
box_and_whisker(df_in, col_name)

count_outliers(df_in, col_name)

"""#Analisis Inferencial: análisis de Varianza (ANOVA)

"""

from scipy.stats import f_oneway
from scipy import stats as sst

anova=sst.f_oneway(df_cuit_personas['credito_en_mora'][df_cuit_personas['tipo_personeria'] == "MASC"],
                df_cuit_personas['credito_en_mora'][df_cuit_personas['tipo_personeria'] == 'FEM']) 
             

#print('Se rechaza la Hipótesis Nula de igualdad de todas la medias, si p valor < Alpha ')

anova
Nombres = ["Estadístico F/Ratio", "p-value"]
pd.Series(anova, index = Nombres)

#Rechazamos la Hipótesis nula(todas las medias son iguales)lo cual se explica por el valor que arroja el p-value=0

"""# Trabajamos con una muestra de 16588 que surge de la fórmula para estimar muestras."""

N=16588
df_muestra= df_cuit_personas.sample(n=N, random_state= 1234).reset_index(drop=True)

df_muestra.columns

# Resta codificar la unica variable dicotomica que figura aún en string:
df_muestra["pers_code"] = np.where(df_muestra["tipo_personeria"].str.contains("FEM"), 1, 0)

df_muestra=pd.get_dummies(df_muestra,columns=["rango_edades"],drop_first=True)

df_muestra.info()

dummies=df_muestra[['rango_edades_40 a 50 años',
       'rango_edades_Hasta 30 años', 'rango_edades_Mas de 50 años']]

variables_dicotomicas=df_muestra[["pers_code","tiene_garantias","tiene_mora_en_Banca_Privada",
                        "tiene_mora_en_Banca_Publica",'Tiene_mora_en_Otras_Entidades']]
variables_numericas=df_muestra[["cred_total","monto_garantias","cant_cuentas","credito_en_mora"]]

y=df_muestra["moroso"]
X=X=pd.concat([variables_dicotomicas,variables_numericas,dummies], axis=1)

dflog=df_muestra[["cred_total","credito_en_mora", "monto_garantias"]] # se toma el logaritmo de las variables numéricas relacionadas con montos para suavizar el efecto de la escala
for column in dflog.columns:
    try:
        dflog[column] = np.where(dflog[column]>0,np.log10(dflog[column]),0)
    except (ValueError, AttributeError):
        pass

X.columns

X.drop(columns=["credito_en_mora","cred_total", 'monto_garantias'],axis=1,inplace=True)
X=pd.concat([X,dflog],axis=1)

X.sample(5)

# Evaluaremos los modelos empleando validación cruzada de K iteraciones
# Definimos el f2_score con un beta=2 , lo que permite medir el Recall (proporción de TP sobre el total de Positive)
def f2_score(y_true, y_pred):
	return fbeta_score(y_true, y_pred, beta=2)

def evaluate_model_f2(X, y, model):
	# Definimos el tipo de conformación de los pliegues (fold/interaciones)
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1234)
	# Definimos la métrica de evaluación del modelo
	metric = make_scorer(f2_score)
	# Evaluación del modelo
	scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)
	return mean(scores), std(scores)
 
def evaluate_model_f1(X, y, model):
	# Definimos el tipo de conformación de los pliegues (fold/interaciones)
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1234)
	# Definimos la métrica de evaluación del modelo
	metric = make_scorer(f1_score)
	# Evaluación del modelo
	scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)
	return mean(scores), std(scores)
 
def evaluate_model_acc(X, y, model):
	# Definimos el tipo de conformación de los pliegues (fold/interaciones)
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1234)
	# Definimos la métrica de evaluación del modelo
	metric = make_scorer(accuracy_score)
	# Evaluación del modelo
	scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)
	return mean(scores), std(scores)
 
def evaluate_model_prec(X, y, model):
	# Definimos el tipo de conformación de los pliegues (fold/interaciones)
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1234)
	# Definimos la métrica de evaluación del modelo
	metric = make_scorer(precision_score)
	# Evaluación del modelo
	scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)
	return mean(scores), std(scores)
 
def evaluate_model_rec(X, y, model):
	# Definimos el tipo de conformación de los pliegues (fold/interaciones)
	cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1234)
	# Definimos la métrica de evaluación del modelo
	metric = make_scorer(recall_score)
	# Evaluación del modelo
	scores = cross_val_score(model, X, y, scoring=metric, cv=cv, n_jobs=-1)
	return mean(scores), std(scores)

X.info()

y.value_counts()

"""# MODELO DE RANDOM FOREST"""

test_size = 0.20
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=22,stratify=y)

X_train.shape

model_RF1 = RandomForestClassifier()
model_RF1.fit(X_train, y_train)
pred_train = model_RF1.predict(X_train)
pred_test = model_RF1.predict(X_test)

f2_cv=evaluate_model_f2(X_train, y_train, model_RF1)
f1_cv=evaluate_model_f1(X_train, y_train, model_RF1)
acc_cv=evaluate_model_acc(X_train, y_train, model_RF1)
prec_cv=evaluate_model_prec(X_train, y_train, model_RF1)
rec_cv=evaluate_model_rec(X_train, y_train, model_RF1)
print(f2_cv)
print(f1_cv)
print(acc_cv)
print(prec_cv)
print(rec_cv)

from sklearn.metrics import plot_confusion_matrix
model_RF1.fit(X_train, y_train)
pred_test = model_RF1.predict(X_test)
plot_confusion_matrix(estimator=model_RF1, X=X_test, y_true=y_test,
                      cmap='Blues')

#Get numerical feature importances 
importances = list(model_RF1.feature_importances_)

#List of tupples with variable and importance
feature_importances = [(feature, round(importance, 2)) for feature,
                      importance in zip(X, importances)]
#Sort the feature importances by most important first
feature_importances = sorted(feature_importances, key = lambda x: x[1], 
                            reverse = True)
#Print out the feature and importances
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];

"""Observamos que la mayor parte es explicada por el crédito en mora. La variable objetivo está construida a partir de la misma, lo que puede generar problema de endogeneidad. Vamos a construir un segundo modelo quitando dicha variable y observar los nuevos resultados.

# Random Forest sin crédito en mora
"""

x= X.drop("credito_en_mora", axis=1) # la x (minúscula tiene dropeado el crédito en mora)

x.head()

test_size = 0.20
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size,random_state=22,stratify=y)

model_RF2 = RandomForestClassifier()
model_RF2.fit(x_train, y_train)
pred_train = model_RF2.predict(x_train)
pred_test = model_RF2.predict(x_test)

f2_cv=evaluate_model_f2(x_train, y_train, model_RF2)
f1_cv=evaluate_model_f1(x_train, y_train, model_RF2)
acc_cv=evaluate_model_acc(x_train, y_train, model_RF2)
prec_cv=evaluate_model_prec(x_train, y_train, model_RF2)
rec_cv=evaluate_model_rec(x_train, y_train, model_RF2)
print(f2_cv)
print(f1_cv)
print(acc_cv)
print(prec_cv)
print(rec_cv)

Random_forest_scores=[round(f2_cv[0],2),round(f1_cv[0],2),round(acc_cv[0],2),round(prec_cv[0],2),round(rec_cv[0],2)]
print(Random_forest_scores)
Scores=["f2_score","f1_score","accuracy_score","precision_score","recall_score"]

from sklearn.metrics import plot_confusion_matrix
model_RF2.fit(x_train, y_train)
pred_test = model_RF2.predict(x_test)
plot_confusion_matrix(estimator=model_RF2, X=x_test, y_true=y_test,
                      cmap='Blues')

#Get numerical feature importances 
importances = list(model_RF2.feature_importances_)

#List of tupples with variable and importance
feature_importances = [(feature, round(importance, 2)) for feature,
                      importance in zip(X, importances)]
#Sort the feature importances by most important first
feature_importances = sorted(feature_importances, key = lambda x: x[1], 
                            reverse = True)
#Print out the feature and importances
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];

"""Al observar la importancia de cada variable dentro del modelo, se puede concluir que el tipo de entidad donde tiene la mora resulta altamente de interés, siendo sobre todo mayor en "otro tipo de entidades". El monto de interés también posee una importancia significativa.

# MODELO DE REGRESION LOGISTICA MÚLTIPLE SIN CRÉDITO EN MORA
"""

test_size = 0.20 # les cambie el nombre de x e y para que el modelo no las confunda
x_training, x_testing, y_training, y_testing = train_test_split(x, y, test_size=test_size, random_state=22,stratify=y)

model_LR2 = LogisticRegression()
model_LR2.fit(x_training, y_training)
pred_training = model_LR2.predict(x_training)
pred_testing = model_LR2.predict(x_testing)

f2_cv=evaluate_model_f2(x_training, y_training, model_LR2)
f1_cv=evaluate_model_f1(x_training, y_training, model_LR2)
acc_cv=evaluate_model_acc(x_training, y_training, model_LR2)
prec_cv=evaluate_model_prec(x_training, y_training, model_LR2)
rec_cv=evaluate_model_rec(x_training, y_training, model_LR2)
print(f2_cv)
print(f1_cv)
print(acc_cv)
print(prec_cv)
print(rec_cv)

Reg_log_scores=[round(f2_cv[0],3),round(f1_cv[0],3),round(acc_cv[0],3),round(prec_cv[0],3),round(rec_cv[0],3)]
print(Reg_log_scores)

Scores_dataframe=pd.DataFrame(list(zip(Reg_log_scores,Random_forest_scores)),columns=["Regresión Logistica","Random Forest Classifer"],index=Scores)

Scores_dataframe

from sklearn.metrics import plot_confusion_matrix
model_LR2.fit(x_training, y_training)
pred_test = model_LR2.predict(x_testing)
plot_confusion_matrix(estimator=model_LR2, X=x_testing, y_true=y_testing,
                      cmap='Blues')



"""# MODELO LOGISTIC SIN CRÉDITO EN MORA PARA VER LA IMPORTANCIA DE LAS VARIABLES"""

x.head() #partimos de nuestra x con datos escalados y sin crédito en mora

test_size = 0.20 # les cambie el nombre de x e y para que el modelo no las confunda
x_training, x_testing, y_training, y_testing = train_test_split(x, y, test_size=test_size, random_state=22,stratify=y)

x_training = sm.add_constant(x_training, prepend=True)
modeloLR2 = sm.Logit(endog=y_training, exog=x_training,)
modeloLR2 = modeloLR2.fit()
print(modeloLR2.summary())

"""De la salida anterior (con un nivel de error de 0,05) se puede observar que las variables relacionadas a la garantía, y los rangos etarios resultan no significativos. Contrariamente el tipo de personería, el tipo de institución donde la persona tiene la deuda, la cantidad de cuentas y el crédito total resultan significativos estadísticamente para el modelo."""

x_testing = sm.add_constant(x_testing, prepend=True)
pred = modeloLR2.predict(exog = x_testing)
pred_test = np.where(pred<0.5, 0, 1)

pred_test

a1 = accuracy_score(y_testing,pred_test) # Proporción de TP y TN sobre el total de observaciones
f1 = f1_score(y_testing, pred_test) # Para un B=1 , es la media entre la precisión y el recall.
f2 = f2_score(y_testing, pred_test) # Para un B=2, da mayor peso al Recall
p1 = precision_score(y_testing, pred_test) # Precisión, es de decir de aquellos calificados como positivos, cuáles son TruePositive?
r1 = recall_score(y_testing, pred_test) # De aquellos que son positivos, cuales fueron calificados como Positivos.
print("accuracy score: %.3f" % a1)
print("f1 score: %.3f" % f1)
print("f2 score: %.3f" % f2)
print("precision score: %.3f" % p1)
print("recall score: %.3f" % r1)

logit_scores=[round(f2,3),round(f1,3),round(a1,3),round(p1,3),round(r1,3)]

#Scores_dataframe["Logit"]=logit_scores

#Scores_dataframe

print(model_LR2.get_params())

"""# COMPARACIÓN ENTRE MODELOS Y CONCLUSIONES GENERALES

Modelo 1: se utiliza un modelo random forest incorporando todas la variables de nuestro dataset. El modelo tiene un desempeño alto en todas las métricas de evaluación definidas. Al observar la importancia que tiene cada variable dentro del modelo, se obser el "crédito en mora" es la principal. Esto nos puede mostrar que existe un problema a la hora de ver que variables incorporamos como explicativas, ya que nuestra variable objetivo fue construida en base a esta. Es por todo lo anterior que se realiza otra prueba quitando esta variable.

Modelo 2: Se vuelve a realizar un modelo de random forest, esta vez sin la variable "crédito en mora". Se observa un desempeño menor que en el primer caso, sin embargo, en un elevado valor. Si nos guiamos por la métrica del recall para comparar nuestros modelos, vemos que el mismo indica un 90 %. Al observar además las matriz de confusión, el modelo tiene buen desempeño para predecir tanto morosos como no morosos.

Modelo 3: Se define un modelo de regresion logistica. En la misma se utilizan las mismas variables de entrada que en modelo número 2, dejando afuera "crédito en mora". Si utilizamos la medida de recall para comparar entre modelos vemos que es tiene un desempeño aún mejor que el modelo dos, siendo del 95%. En este caso al no poder observar la importancia de cada variable, se recurre a un 4 modelo tipo logit para poder captar esto.

Modelo 4: Para finalizar, se realiza un modelo logit, de la librería stat model. De variable de entrada se utilizan las mismas features que el caso dos y tres. Se obtiene un recall del 94%, levemente por debajo de la regresion logistica. En cuanto a la significando de las variables del modelo, sólo las relacionada a las garantías no resultan de interés

# Gridsearch y nuevo entrenamiento modelo de Regresión Logística
"""

# Definir modelo
model = LogisticRegression()

# Configurar el proceso de cross-validation "RepeatedStratifiedKFold" (con 10 folds y 3 repetciciones)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

# Definimos del espacio de búsqueda
# Generamos un diccionaro que contenga los sigientes hiperparámetros: solver, penalty, C
space = dict()
space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']
space['penalty'] = ['none', 'l1', 'l2', 'elasticnet']
space['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]
space["class_weight"]=["balanced",None]

# Definimos la métrica que emplearemos (usar la función "make_scorer")
metric = make_scorer(recall_score)

# Definimos la búsqueda especificando los argumentos anteriores: model, space, scoring, cv
# (adicionalmente incorporar el argumento n_jobs=-1; ver el help de la función) 
search = GridSearchCV(model, space, scoring=metric, n_jobs=-1, cv=cv)

# Ejecutamos la búsqueda: usar al función "search.fit" con los conjuntos de entrenamiento
result = search.fit(x_training, y_training)

print('Mejor Score: %.3f' % result.best_score_)
print('Mejores hiperparámetros: %s' % result.best_params_)



"""# MODELO DE REGRESION LOGISTICA MÚLTIPLE CON PARAMETROS OPTMIZADOS"""

test_size = 0.20 # les cambie el nombre de x e y para que el modelo no las confunda
x_training, x_testing, y_training, y_testing = train_test_split(x, y, test_size=test_size, random_state=22,stratify=y)

model_LR2opt = LogisticRegression(C= 0.1, class_weight= 'balanced', penalty= 'l2', solver='newton-cg')
model_LR2opt.fit(x_training, y_training)
pred_training = model_LR2opt.predict(x_training)
pred_testing = model_LR2opt.predict(x_testing)

f2_cv_opt=evaluate_model_f2(x_training, y_training, model_LR2opt)
f1_cv_opt=evaluate_model_f1(x_training, y_training, model_LR2opt)
acc_cv_opt=evaluate_model_acc(x_training, y_training, model_LR2opt)
prec_cv_opt=evaluate_model_prec(x_training, y_training, model_LR2opt)
rec_cv_opt=evaluate_model_rec(x_training, y_training, model_LR2opt)
print(f2_cv_opt)
print(f1_cv_opt)
print(acc_cv_opt)
print(prec_cv_opt)
print(rec_cv_opt)

Reg_log_scores_OPTMIZADO=[round(f2_cv_opt[0],3),round(f1_cv_opt[0],3),round(acc_cv_opt[0],3),round(prec_cv_opt[0],3),round(rec_cv_opt[0],3)]
print(Reg_log_scores_OPTMIZADO)

Scores_dataframe["Reg Logística con Optimizacion Parametros"]=Reg_log_scores_OPTMIZADO

Scores_dataframe

from sklearn.metrics import plot_confusion_matrix
model_LR2.fit(x_training, y_training)
pred_test = model_LR2.predict(x_testing)
plot_confusion_matrix(estimator=model_LR2, X=x_testing, y_true=y_testing,
                      cmap='Blues')

"""Podemos observar cómo al optimizar el modelo de regresión multiple a través un GridSearch logramos alcanzar un recall_score del 0,999, siendo el mejor resultado en base de nuestra problema de interés."""

